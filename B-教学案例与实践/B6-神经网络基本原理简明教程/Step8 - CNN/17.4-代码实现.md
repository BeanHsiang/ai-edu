Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https：//github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 17.4 代码实现

### 17.4.1 卷积核的实现

卷积核，实际上和全连接层一样，是权重矩阵加偏移向量的组合，

```Python
class ConvWeightsBias(WeightsBias_2_1):
    def __init__(self, output_c, input_c, filter_h, filter_w, init_method, optimizer_name, eta):
        self.FilterCount = output_c
        self.KernalCount = input_c
        self.KernalHeight = filter_h
        self.KernalWidth = filter_w
        self.init_method = init_method
        self.optimizer_name = optimizer_name
        self.learning_rate = eta

    def Initialize(self, folder, name, create_new):
        self.WBShape = (self.FilterCount, self.KernalCount, self.KernalHeight, self.KernalWidth)        
        ......

    def CreateNew(self):
        self.W = ConvWeightsBias.InitialConvParameters(self.WBShape, self.init_method)
        self.B = np.zeros((self.FilterCount, 1))
......
```
- FilterCount - 过滤器数量，对应输出通道数
- KernalCount - 卷积核数量，对应输入通道数
- KernalHeight, KernalWidth - 过滤器的高度宽度，一般同为奇数
- init_method - 在使用ReLU激活函数时，一般用MSRA/HE初始化
- optimizer_name - 优化器的名称，如Adam等
- eta - 即学习率Learning Rate，在初始化优化器时使用

在初始化函数中，会根据四个参数定义WBShape，然后在CreateNew函数中，创建相应形状的Weights和Bias。

### 17.4.2 卷积层的实现 - 标准方法

##### 前向计算

按照卷积前向计算的定义，先对输入矩阵做padding，然后做思维卷积运算。

```Python
    def forward_numba(self, x, train=True):
        self.x = x
        # padding
        if self.padding > 0:
            self.padded = np.pad(self.x, ((0,0), (0,0), (self.padding,self.padding), (self.padding,self.padding)), 'constant')
        else:
            self.padded = self.x
        # convolution
        self.z = jit_conv_4d(self.padded, self.WB.W, self.WB.B, self.output_height, self.output_width, self.stride)
        return self.z
```

读者可能会奇怪，为什么前向计算函数命名为forward_numba？并且四维卷积的函数名是jit_conv_4d？

这是因为在Numpy中，如果用for循环的方式来实现卷积计算，执行速度会非常非常慢。有一个开源的项目叫做numba，它会把python代码编译成C代码，这样会大大提高运算速度。但是需要在函数前面加一行：
```Python
@nb.jit(nopython=True)
```
这样在执行之前会先编译，对该函数的实现也有要求，必须是原始数据类型，比如float，int等等。卷积的代码如下：

```Python
@nb.jit(nopython=True)
def jit_conv_4d(x, weights, bias, out_h, out_w, stride=1):
    # 输入图片的批大小，通道数，高，宽
    assert(x.ndim == 4)
    # 输入图片的通道数
    assert(x.shape[1] == weights.shape[1])  
    batch_size = x.shape[0]
    num_input_channel = x.shape[1]
    num_output_channel = weights.shape[0]
    filter_height = weights.shape[2]
    filter_width = weights.shape[3]
    rs = np.zeros((batch_size, num_output_channel, out_h, out_w))

    for bs in range(batch_size):
        for oc in range(num_output_channel):
            rs[bs,oc] += bias[oc]
            for ic in range(num_input_channel):
                for i in range(out_h):
                    for j in range(out_w):
                        ii = i * stride
                        jj = j * stride
                        for fh in range(filter_height):
                            for fw in range(filter_width):
                                rs[bs,oc,i,j] += x[bs,ic,ii+fh,jj+fw] * weights[oc,ic,fh,fw]
    #end bs
    return rs
```
完全是用C的风格写的代码，而且越像C的话，执行速度越快。

#### 反向传播

```Python
    def backward_numba(self, delta_in, flag):
        # 转换误差矩阵尺寸
        dz_stride_1 = expand_delta_map(...)
        # 计算本层的权重矩阵的梯度
        self._calculate_weightsbias_grad(dz_stride_1)
        # 求本层的输出误差矩阵时，应该用本层的输入误差矩阵互相关计算本层的卷积核的旋转
        dz_padded = np.pad(dz_stride_1, ((0,0),(0,0),(pad_h, pad_h),(pad_w, pad_w)), 'constant')
        # 计算本层输出到下一层的误差矩阵
        delta_out = self._calculate_delta_out(dz_padded, flag)
        #return delta_out
        return delta_out, self.WB.dW, self.WB.dB
```

### 17.4.3 卷积层的实现 - 优化方法

##### 前向计算

```Python
    def forward_img2col(self, x, train=True):
        self.x = x
        FN, C, FH, FW = self.WB.W.shape
        N, C, H, W = x.shape
        out_h = 1 + int((H + 2 * self.padding - FH) / self.stride)
        out_w = 1 + int((W + 2 * self.padding - FW) / self.stride)
        self.col_x = img2col(x, FH, FW, self.stride, self.padding)
        self.col_w = self.WB.W.reshape(FN, -1).T
        out1 = np.dot(self.col_x, self.col_w) + self.WB.B.reshape(-1,FN)
        out2 = out1.reshape(N, out_h, out_w, -1)
        self.z = np.transpose(out2, axes=(0, 3, 1, 2))
        return self.z
```
#### 反向传播

```Python
    def backward_col2img(self, delta_in, layer_idx):
        FN, C, FH, FW = self.WB.W.shape
        dout = np.transpose(delta_in, axes=(0,2,3,1)).reshape(-1, FN)
        self.WB.dB = np.sum(dout, axis=0, keepdims=True).T / self.batch_size
        dW = np.dot(self.col_x.T, dout)
        self.WB.dW = np.transpose(dW, axes=(1, 0)).reshape(FN, C, FH, FW) / self.batch_size
        dcol = np.dot(dout, self.col_w.T)
        delta_out = col2img(dcol, self.x.shape, FH, FW, self.stride, self.padding)
        return delta_out, self.WB.dW, self.WB.dB
```

# 池化层

```Python
class PoolingLayer(CLayer):
    def __init__(self,
                input_shape,    # (input_c, input_h, input_w)
                pool_shape,     # (pool_h, pool_w)
                stride, 
                pool_type):  # MAX, MEAN
        self.num_input_channel = input_shape[0]
        self.input_height = input_shape[1]
        self.input_width = input_shape[2]
        self.pool_height = pool_shape[0]
        self.pool_width = pool_shape[1]
        self.stride = stride
        self.pool_type = pool_type
        self.pool_size = self.pool_height * self.pool_width
        self.output_height = (self.input_height - self.pool_height) // self.stride + 1
        self.output_width = (self.input_width - self.pool_width) // self.stride + 1
        self.output_shape = (self.num_input_channel, self.output_height, self.output_width)
        self.output_size = self.num_input_channel * self.output_height * self.output_width
```

## 成员变量

- num_input_channel, input_height, input_width - 输入数据的通道数，高，宽
- pool_height, pool_width - 池的高和宽（一般高等于宽）
- stride - 池的步长（一般等于池高，也有个别不等的）
- pooling_type - 最大池化还是平均池化（一般用最大池化）
- pool_size - 池的面积（等于池的高x宽）
- output_height, output_width - 输出数据的高和宽（不会改变通道数）

## 成员方法

- forward - 正向计算
- backward - 反向传播
- save_parameters - 保存池化尺寸、类型等设置
- load_parameters - 加载池化尺寸、类型等设置

# 实现要领

## 性能

如果完全按照卷积的定义，其计算过程是逐点相乘，然后结果求和，再移动到下一个区域，重复以上计算过程。当图片大或者卷积核多的时候（一般情况下，大图片都需要更多的卷积核来识别特征），这种重复的逐点计算非常耗时，所以在一些深度学习框架中，使用了下面的这种做法：

<img src='../Images/17/img2col.png'/>

假设Input Features有三个通道，两个卷积核，每个卷积核中需要定义三个滤波器，最后输出最上面的两个output features. 所以上面三层就是传统的卷积计算方式。

我们来看最下面那一层，根据卷积计算的规律（卷积核尺寸和步长），事先把通道一的图片（第三层最左侧）平铺到一个大矩阵中，因为步长要小于卷积核尺寸，所以数据会有重叠，比如那个[1,2,0]的2就在下面的矩阵中出现两次（第一行第2个和第二行第1个），所以这张3x3的图片会变成一个4x4的矩阵。同理，其它两个通道也做相同变换，最后形成一个4x12的大矩阵，里面包含了三个通道的所有输入数据。

再看那个Kernal Matrix，它把2x3x2x2的6个滤波器的数据，变成了12x2的大矩阵，其中12代表一个卷积核内的滤波器值，2代表两个卷积核。

上面这两个矩阵相乘，得到最右侧的Output Features(Matrix)，它的数值和最上层的Output Features的数值完全相同，只要把Output Features Matrix按正确的形状reshape就可以了。

由于对于大型矩阵运算，函数库和GPU都做了优化，所以这种变换带来的性能提高是非常可观的，数据量越大提高的性能越明显，十倍是最低的数量级。

对于池化操作，按定义我们要这样写代码：
```Python
    z = np.zeros((batch_size, input_c, output_h, output_w)).astype(np.float32)
    for b in range(batch_size):
        for c in range(input_c):
            for i in range(output_h):
                i_start = i * pool_stride
                i_end = i_start + pool_h
                for j in range(output_w):
                    j_start = j * pool_stride
                    j_end = j_start + pool_w
                    target_array = x[b,c,i_start:i_end, j_start:j_end]
                    z[b,c,i,j] = target_array.max()
```
如果是2x2的池子，步长为2的话，我们需要每4个点计算一次，虽然比卷积快了4倍，但是还有计算max值，或者mean值，也相当于每个点都遍历一遍，性能极差。

也有类似与卷积优化的方法来计算池化层。比如有以下数据：

<img src='../Images/17/img2col_pool.png'/>

在上面的图片中，我们假设大写字母为池子中的最大元素，并且用max_pool方式。

原始数据先做img2col变换，然后做一次np.max(axis=1)的max计算，会大大增加速度，然后把结果renshape成正确的矩阵即可。做一次大矩阵的max计算，比做4次小矩阵计算要快很多。

## 没有GPU咋办

用纯Python写出来的代码，运行效率比较低。在前面学习的神经网络的代码，经过高度优化，运行一般的只有全连接层的深度学习网络速度非常的快，但是一遇到卷积，立刻呆滞。

- 如果你有GPU，而且是在Linux上，那么NumPy库还是可以跑GPU版的。
- 如果你有GPU，但是是在Windows 10上，NumPy库并不支持，只好想别的办法，比如MXNET的科学计算库。
- 如果你没有GPU，可以看一下Numba这个库，http://numba.pydata.org/，用pip install numba安装

在我们的代码示例中，有一个独立的文件jit_utility.py，里面有很多简单功能函数，如：

- jit_conv_2d
- jit_conv_4d
- max_pool_forward
- max_pool_backward

等等，都是用@nb.jit(nopython=True)装饰器，让这些函数在运行前被编译成C函数库，相当于Python调用C代码来做卷积池化等操作，会比用纯Python快十倍到几十倍。但是也有缺点：

- 对NumPy函数支持有限
- 输入参数不能是自定义类型

Numba也支持GPU，但是要用到Numba定义的一些函数库，才能调用GPU设备。使用Numba可以解燃眉之急，但是在CPU上还是比GPU慢很多，只能跑一些小的卷积模型。
